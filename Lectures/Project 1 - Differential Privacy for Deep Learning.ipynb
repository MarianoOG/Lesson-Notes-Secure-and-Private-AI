{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Differential Privacy for Deep Learning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]( https://colab.research.google.com/github/MarianoOG/Lesson-Notes-Secure-and-Private-AI/blob/master/Lectures/Project%201%20-%20Differential%20Privacy%20for%20Deep%20Learning.ipynb)\n",
    "\n",
    "Previously, we defined perfect privacy as _a query to a database returns the same value even if we remove any person from the database_, and used this intuition in the description of epsilon/delta. In the context of deep learning we have a similar standard. **Training a model on a dataset should return the same model even if we remove any person from the dataset.**\n",
    "\n",
    "We've replaced \"querying a database\" with \"training a model on a dataset\". However, one should note that this adds two points of complexity which database queries did not have:\n",
    "\n",
    "### 1. Do we always know where _people_ are referenced in the dataset?\n",
    "\n",
    "We need to treat each training example as a single, separate person. Strictly speaking, this is not true as some training examples have no relevance to people and others may have multiple or partial relevance to people (consider an image with multiple or no people contained within it).\n",
    "\n",
    "### 2. Neural models rarely never train to the same output model\n",
    "\n",
    "Even on identical data the output model is different. Several proposals have been made to solve this problem. We're going to focus on one of the most popular, PATE.\n",
    "\n",
    "## Example Scenario: A Health Neural Network\n",
    "\n",
    "You have a large collection of images from the patients of a hospital but you don't know what's in them. You realize that you can reach out to partner hospitals which do have annotated data. It is your hope to train a classifier on their datasets so that you can automatically label your own. While these hospitals are interested in helping, they have privacy concerns regarding information about their patients. So you will use the following technique to train a classifier which protects the privacy of patients in the other hospitals.\n",
    "\n",
    "1. You'll ask each of the hospitals to train a model on their own datasets (all of which have the same kinds of labels).\n",
    "2. You'll then use each of the partner models to predict on your local dataset, generating labels for each of your datapoints.\n",
    "3. For each local data point (now with labels), you will perform a _max_ function, where _max_ is the most frequent label across the labels to obtain the final label. We will need to add Laplacian noise to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "4. Finally, we will retrain a new model on our local dataset which now has labels. This will be our final _DP_ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced agreement = 0.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 11.756462732485105\n",
      "\n",
      "Forced agreement = 2.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 10.567352334004832\n",
      "\n",
      "Forced agreement = 4.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 8.900097381461716\n",
      "\n",
      "Forced agreement = 6.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 6.81765626580294\n",
      "\n",
      "Forced agreement = 8.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 4.877227210477253\n",
      "\n",
      "Forced agreement = 10.0 %\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 0.9029013677789843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this proyect:\n",
    "We will assume we have the labels from the partner models so we will start on step 3 of the example scenario.\n",
    "We will be using data from 100 partner hospitals and each of them will provide 100 training toy examples.\n",
    "Each label is chosen from a set of 10 possible labels (categories) for each image.\n",
    "Then you're going to use the PATE method to analyze how much privacy (epsilon) is this database leaking.\n",
    "'''\n",
    "import numpy as np\n",
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "# Variables of the toy database\n",
    "num_teachers = 100    # we're working with 10 partner hospitals\n",
    "num_examples = 100    # the size of OUR dataset\n",
    "num_labels   = 10     # number of lablels for our classifier\n",
    "\n",
    "# Predictions from hospitals on each of their images (partner labels)\n",
    "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int).transpose(1,0)\n",
    "\n",
    "# Adding noise to the labels\n",
    "real_result = list()\n",
    "private_result = list()\n",
    "for an_image in preds:\n",
    "    # Calculate real label (not private)\n",
    "    label_counts = np.bincount(an_image, minlength=num_labels) \n",
    "    real_result.append(np.argmax(label_counts))\n",
    "    \n",
    "    # Calculating epsilon value\n",
    "    epsilon = 0.1\n",
    "    beta = 1 / epsilon\n",
    "\n",
    "    # Adding noise\n",
    "    for i in range(len(label_counts)):\n",
    "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "\n",
    "    # Storing private results\n",
    "    private_result.append(np.argmax(label_counts))\n",
    "\n",
    "# Arranging the datasets to be used by PATE\n",
    "preds = preds.transpose(1,0)\n",
    "indices = np.asarray(private_result)\n",
    "\n",
    "# PATE analysis for several agreement levels (forced)\n",
    "for i in range(6):\n",
    "    k = i*2\n",
    "    preds[:,0:k] *= 0\n",
    "    \n",
    "    dep, ind = pate.perform_analysis(teacher_preds=preds,indices=indices,noise_eps=0.1,delta=1e-5,moments=20)\n",
    "\n",
    "    assert dep < ind\n",
    "    \n",
    "    print(\"Forced agreement =\", 100*k/num_examples, \"%\")\n",
    "    print(\"Data Independent Epsilon:\", ind)\n",
    "    print(\"Data Dependent Epsilon:\", dep)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when there's more agreement between the teacher predictions (partner models) the epsilon data dependent value is reduced drastically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
