{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Differencial Privacy\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]( https://colab.research.google.com/github/MarianoOG/Lesson-Notes-Secure-and-Private-AI)\n",
    "\n",
    "First of all, we talk about a **differencial atack** when an attaker is able to obtain information from a particular individual or group from a database by making a query to the entire database and to a sub-database derived from the original (with some data removed). Then, the attaker compares the difference between the outputs of this queries and with this results is able to obtain information about the individual or group. We say that a database is **differencially private** when this kind of attacks are not succesfull.\n",
    "\n",
    "In this section we will first explore how a query can reveal information from an individual or group in the database. Then we will define differencial privacy with the help of the concept of sensitivity. And finally we will explore differencial privacy in the context of deep learning.\n",
    "\n",
    "## Lesson 1: Simple Database Queries\n",
    "\n",
    "We're going to make a toy database (for educational reasons) by initializing a random list of 1s and 0s which will represent sensitive data from a population. The number of entries directly corresponds to the number of people in our database.\n",
    "\n",
    "**Key to the definition of differenital privacy is the ability to ask the question \"When querying a database, if I removed someone from the database, would the output of the query be any different?\"**. Thus, in order to check this, we will create sub-databases that we will call *parallel databases* which are databases with one entry removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1],\n",
      "       dtype=torch.uint8)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this first project, we will create parallel databases \"pdb\" from the main database \"db\".\n",
    "'''\n",
    "\n",
    "# First we import the needed libraries\n",
    "import torch\n",
    "\n",
    "# Function to create database with n entries (number of people)\n",
    "def create_db(n):\n",
    "    return torch.rand(n) > 0.5\n",
    "\n",
    "# Funtion to create 1 parallel database removing one particual index\n",
    "def get_parallel_db(db, remove_index):\n",
    "    return torch.cat((db[0:remove_index], \n",
    "                      db[remove_index+1:]))\n",
    "\n",
    "# Funtion to create all parallel databases from a database\n",
    "def get_parallel_dbs(db):\n",
    "    parallel_dbs = list()\n",
    "    for i in range(len(db)):\n",
    "        pdb = get_parallel_db(db, i)\n",
    "        parallel_dbs.append(pdb)\n",
    "    return parallel_dbs\n",
    "\n",
    "# Function to create database and parallel databases\n",
    "def create_db_and_parallels(n):\n",
    "    db = create_db(n)\n",
    "    pdbs = get_parallel_dbs(db)\n",
    "    return db, pdbs\n",
    "\n",
    "# Creating actual database and parallel databases\n",
    "db, pdbs = create_db_and_parallels(20)\n",
    "\n",
    "# Printing results:\n",
    "print(db)         # Database\n",
    "print(len(pdbs))  # Number of parallel databases\n",
    "# print(pdbs)      # Parallel databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Towards Evaluating The Differential Privacy of a Function\n",
    "\n",
    "We want to be able to query our database and evaluate whether or not the result of the query is leaking private information. As mentioned previously, **this is about evaluating whether the output of a query changes when we remove someone from the database**. Specifically, we want to evaluate the *maximum* amount the query changes when someone is removed (maximum over all possible people who could be removed). \n",
    "\n",
    "In order to evaluate how much privacy is leaked, first we are going to define functions that will be our queries to the database. Then, we will measure the difference between each parallel db's query result and the query result for the entire database. Finally, we will calculate the maximum value along those results. This value is called sensitivity. **Sensitivity is measuring how much the output of the query changes when a person is removed from the database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum query\n",
      "N = 1 - sentivity = 0\n",
      "N = 2 - sentivity = tensor(1.)\n",
      "N = 3 - sentivity = tensor(1.)\n",
      "N = 4 - sentivity = tensor(1.)\n",
      "N = 5 - sentivity = tensor(1.)\n",
      "N = 6 - sentivity = tensor(1.)\n",
      "N = 7 - sentivity = tensor(1.)\n",
      "N = 8 - sentivity = tensor(1.)\n",
      "N = 9 - sentivity = tensor(1.)\n",
      "Some results are not deterministic they will change if you re-run this cell.\n",
      "\n",
      "Treshold query\n",
      "N = 10 - sentivity = tensor(1.)\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = tensor(1.)\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = tensor(1.)\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = 0\n",
      "N = 10 - sentivity = tensor(1.)\n",
      "N = 10 - sentivity = 0\n",
      "Even when the number of samples are the same the sensitity of this function varies\n",
      "\n",
      "Mean query\n",
      "N = 10 - sentivity = tensor(0.0556)\n",
      "N = 20 - sentivity = tensor(0.0316)\n",
      "N = 30 - sentivity = tensor(0.0184)\n",
      "N = 40 - sentivity = tensor(0.0135)\n",
      "N = 50 - sentivity = tensor(0.0110)\n",
      "N = 60 - sentivity = tensor(0.0088)\n",
      "N = 70 - sentivity = tensor(0.0087)\n",
      "N = 80 - sentivity = tensor(0.0065)\n",
      "N = 90 - sentivity = tensor(0.0065)\n",
      "Observe how if the database is bigger the sensitivy is reduced.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this project we will:\n",
    "1) Create a list of queries (sum, threshold and mean)\n",
    "2) Calculate sensitivy for each one\n",
    "'''\n",
    "    \n",
    "# Sum query function\n",
    "def query_sum(db):\n",
    "    return db.sum().float()\n",
    "\n",
    "# Threshold query function\n",
    "def query_threshold(db, threshold=5):\n",
    "    return (db.sum() > threshold).float()\n",
    "\n",
    "# Mean query function\n",
    "def query_mean(db):\n",
    "    return db.float().mean()\n",
    "\n",
    "# Calculate sensitivity\n",
    "def sensitivity(query, n):\n",
    "    db, pdbs = create_db_and_parallels(n)\n",
    "    db_result = query(db)\n",
    "    max_distance = 0\n",
    "    for pdb in pdbs:\n",
    "        pdb_result = query(pdb)\n",
    "        db_distance = torch.abs(pdb_result - db_result)\n",
    "        if(db_distance > max_distance):\n",
    "            max_distance = db_distance\n",
    "    return max_distance\n",
    "\n",
    "# Sensitivity for sum\n",
    "print(\"Sum query\")\n",
    "for i in range(1,10):\n",
    "    s = sensitivity(query_sum, i)\n",
    "    print(\"N =\", i, \"- sentivity =\", s)\n",
    "print(\"Some results are not deterministic they will change if you re-run this cell.\")\n",
    "print(\"\")\n",
    "    \n",
    "# Sensitivity for threshold\n",
    "print(\"Treshold query\")\n",
    "for i in range(15):\n",
    "    s = sensitivity(query_threshold, 10)\n",
    "    print(\"N =\", 10, \"- sentivity =\", s)\n",
    "print(\"Even when the number of samples are the same the sensitity of this function varies\")\n",
    "print(\"\")\n",
    "\n",
    "# Sensitivity for mean\n",
    "print(\"Mean query\")\n",
    "for i in range(1,10):\n",
    "    s = sensitivity(query_mean, i*10)\n",
    "    print(\"N =\", i*10, \"- sentivity =\", s)\n",
    "print(\"Observe how if the database is bigger the sensitivy is reduced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - A Basic Differential Attack\n",
    "\n",
    "Now we measure how sensible each function is for those particular cases but none of the functions we've looked at so far are differentially private. We will discover how to obtain information from them using a basic differential attack.\n",
    "\n",
    "The most basic type of attack can be done as follows: let's say we wanted to figure out a specific person's value in the database. All we would have to do is query for the sum of the entire database and then the sum of the entire database without that person. Something similar will happen with the mean and threshold queries that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private value: tensor(0, dtype=torch.uint8)\n",
      "\n",
      "Differencing attacks:\n",
      "\tSum query = tensor(0.)\n",
      "\tMean query = tensor(-0.0048)\n",
      "\tThreshold query = tensor(0.)\n",
      "\n",
      "As you can see, the basic sum query is not differentially private at all!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this project we will:\n",
    "1) Construct a database.\n",
    "2) Demonstrate how two different sum queries can expose the value of the person represented by row 10.\n",
    "'''\n",
    "\n",
    "# Generate a database with only one parallel database (in index 10)\n",
    "db = create_db(100)\n",
    "pdb = get_parallel_db(db, remove_index=10)\n",
    "\n",
    "# Printing reults:\n",
    "print(\"Private value:\", db[10])\n",
    "print(\"\")\n",
    "print(\"Differencing attacks:\")\n",
    "print(\"\\tSum query =\", query_sum(db) - query_sum(pdb))\n",
    "print(\"\\tMean query =\", query_mean(db) - query_mean(pdb))\n",
    "print(\"\\tThreshold query =\", query_threshold(db, 50) - query_threshold(pdb, 50))\n",
    "print(\"\")\n",
    "print(\"As you can see, the basic sum query is not differentially private at all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential privacy always requires a form of randomness added to the query. \n",
    "\n",
    "One technique is to add randomness to each person's response. Take in consideration that when you induce noise to each person's response you will obtain a skewed result. However, if the amount of noise introduced is known its posible to calculate an aproximation of the real result. It should be noted that, especially when we only have a few samples, this comes at the cost of accuracy. The greater the privacy protection the less accurate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 100 \tWithout Noise = tensor(0.4400) \tWith Noise = tensor(0.4750)\n",
      "N = 1000 \tWithout Noise = tensor(0.4650) \tWith Noise = tensor(0.4450)\n",
      "N = 10000 \tWithout Noise = tensor(0.4960) \tWith Noise = tensor(0.4955)\n",
      "N = 100000 \tWithout Noise = tensor(0.5006) \tWith Noise = tensor(0.4998)\n",
      "N = 1000000 \tWithout Noise = tensor(0.4999) \tWith Noise = tensor(0.5001)\n",
      "N = 10000000 \tWithout Noise = tensor(0.5001) \tWith Noise = tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this project we will:\n",
    "1) Create a function that will introduce noise to the database \n",
    "2) Create a function that will calculate an aproximation of the real result from a skewed result.\n",
    "2) Demostrate how the effect of the induced noise is diminished when the data set is bigger.\n",
    "'''\n",
    "\n",
    "def add_noise(db, noise=0.2):\n",
    "    selected = (torch.rand(len(db)) > noise).float() # Will decide if the entry will be real or random\n",
    "    random_answer = (torch.rand(len(db)) > 0.5).float()\n",
    "    augmented_database = db.float() * selected + random_answer * (1 - selected)\n",
    "    return augmented_database\n",
    "    \n",
    "def deskew(sk_result, noise=0.2):\n",
    "    return ((sk_result / noise) - 0.5) * noise / (1 - noise)\n",
    "\n",
    "for i in range(2,8):\n",
    "    db = create_db(10**i)\n",
    "    true_result = query_mean(db)\n",
    "    augmented_database = add_noise(db)\n",
    "    sk_result = query_mean(augmented_database)\n",
    "    private_result = deskew(sk_result)\n",
    "    print(\"N =\", 10**i, \"\\tWithout Noise =\", true_result, \"\\tWith Noise =\", private_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous method of adding noise was called **Local Differentail Privacy** because we added noise to each datapoint individually. This is necessary for some situations wherein the data is SO sensitive that individuals do not trust noise to be added later. However, it comes at a very high cost in terms of accuracy. \n",
    "\n",
    "Alternatively we can add noise _after_ data has been aggregated by a function. This kind of noise allows us to perform differential privacy on smaller groups of individuals with lower amounts of noise. Becasue of this accuracy will be less affected. However, participants must be able to trust that no-one looked at their datapoints _before_ the aggregation took place. This method is called **Global Differential Privacy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 10 \tWithout Noise = tensor(5.) \tWith Noise = tensor([4.5829])\n",
      "N = 20 \tWithout Noise = tensor(7.) \tWith Noise = tensor([6.3510])\n",
      "N = 30 \tWithout Noise = tensor(17.) \tWith Noise = tensor([17.4437])\n",
      "N = 40 \tWithout Noise = tensor(18.) \tWith Noise = tensor([19.3799])\n",
      "N = 50 \tWithout Noise = tensor(25.) \tWith Noise = tensor([22.7138])\n",
      "N = 60 \tWithout Noise = tensor(34.) \tWith Noise = tensor([35.6781])\n",
      "N = 70 \tWithout Noise = tensor(35.) \tWith Noise = tensor([35.5389])\n",
      "N = 80 \tWithout Noise = tensor(33.) \tWith Noise = tensor([36.1455])\n",
      "N = 90 \tWithout Noise = tensor(42.) \tWith Noise = tensor([44.7209])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this project we will:\n",
    "1) Create a function that will introduce noise to the result of the query (global differencial privacy)\n",
    "'''\n",
    "\n",
    "def M(true_result, noise=0.2):\n",
    "    sk = (torch.rand(1)).float()*noise - noise/2.0\n",
    "    private_result = true_result + true_result*sk\n",
    "    return private_result\n",
    "\n",
    "for i in range(1, 10):\n",
    "    db = create_db(10*i)\n",
    "    true_result = query_sum(db)\n",
    "    private_result = M(true_result)\n",
    "    print(\"N =\", 10*i, \"\\tWithout Noise =\", true_result, \"\\tWith Noise =\", private_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 - The Formal Definition of Differential Privacy\n",
    "\n",
    "This definition is a measure of how much privacy is afforded by a query M. Specifically, it's a comparison between running the query M on a database (x) and a parallel database (y). As a reminder, parallel databases are defined to be the same as a full database (x) with one entry/person removed.\n",
    "\n",
    "[![Image From: \"The Algorithmic Foundations of Differential Privacy\" - Cynthia Dwork and Aaron Roth](dp_formula.png \"Title\")](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)\n",
    "\n",
    "This theorem is called \"epsilon delta\" differential privacy. It says: for all parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon plus a probability delta.\n",
    "\n",
    "### Epsilon\n",
    "\n",
    "**Epsilon Zero:** If a query satisfied this inequality with epsilon 0, then that would mean that the query for all parallel databases outputed the exact same value as the full database. If the sensitivity of a query is 0, then the epsilon value also happened to be zero.\n",
    "\n",
    "**Epsilon One:** If a query satisfied this inequality with epsilon 1, then the maximum distance between the two random distributions M(x) and M(y) is 1.\n",
    "\n",
    "### Delta\n",
    "\n",
    "Delta is basically the probability that epsilon breaks. Sometimes the epsilon value is different for some queries than for others, delta represents this difference using probabilities. Note that this expression doesn't represent the full tradeoff between epsilon and delta.\n",
    "\n",
    "## Lesson 5: How To Add Noise for Global Differential Privacy\n",
    "\n",
    "In this lesson, we're going to learn about how to take a query and add varying amounts of noise so that it satisfies a certain degree of differential privacy (a certain epsilo-delta values). In particular, we're going to focus on global differential privacy.\n",
    "\n",
    "There are two kinds of noise we can add: **Gaussian Noise** or **Laplacian Noise**. Generally speaking Laplacian is better, but both are still valid.\n",
    "\n",
    "### How much noise should we add?\n",
    "\n",
    "The amount of noise necessary to add to the output of a query is a function of four things:\n",
    "\n",
    "- the type of noise (Gaussian/Laplacian)\n",
    "- the sensitivity of the query/function\n",
    "- the desired epsilon (ε)\n",
    "- the desired delta (δ)\n",
    "\n",
    "**Querying Repeatedly:** If we query the database multiple times we can simply add the epsilons, even if we change the amount of noise and their epsilons are not the same.\n",
    "\n",
    "### Using Laplacian Noise\n",
    "\n",
    "Laplacian noise is increased/decreased according to a _scale_ parameter b. We choose _b_ based on the following formula:\n",
    "\n",
    "b = sensitivity(query) / epsilon\n",
    "\n",
    "In other words, if we set b to be this value, then we know that we will have a privacy leakage of <= epsilon. \n",
    "\n",
    "Laplace function guarantees that delta value is equal to 0 (there are some tunings where we can have very low epsilon where delta is non-zero, but we'll ignore them for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum query\n",
      "N = 10 \tWithout Noise = tensor(6.) \tWith Noise = tensor([135.6311])\n",
      "N = 20 \tWithout Noise = tensor(8.) \tWith Noise = tensor([77.1518])\n",
      "N = 30 \tWithout Noise = tensor(11.) \tWith Noise = tensor([235.4594])\n",
      "N = 40 \tWithout Noise = tensor(19.) \tWith Noise = tensor([22.2522])\n",
      "N = 50 \tWithout Noise = tensor(26.) \tWith Noise = tensor([210.6535])\n",
      "N = 60 \tWithout Noise = tensor(29.) \tWith Noise = tensor([171.1573])\n",
      "N = 70 \tWithout Noise = tensor(34.) \tWith Noise = tensor([47.3793])\n",
      "N = 80 \tWithout Noise = tensor(39.) \tWith Noise = tensor([145.0871])\n",
      "N = 90 \tWithout Noise = tensor(47.) \tWith Noise = tensor([-184.5951])\n",
      "\n",
      "Mean query\n",
      "N = 10 \tWithout Noise = tensor(5.) \tWith Noise = tensor([-603.0083])\n",
      "N = 20 \tWithout Noise = tensor(14.) \tWith Noise = tensor([-271.9333])\n",
      "N = 30 \tWithout Noise = tensor(19.) \tWith Noise = tensor([18.8040])\n",
      "N = 40 \tWithout Noise = tensor(20.) \tWith Noise = tensor([140.6546])\n",
      "N = 50 \tWithout Noise = tensor(23.) \tWith Noise = tensor([-286.5993])\n",
      "N = 60 \tWithout Noise = tensor(29.) \tWith Noise = tensor([73.1706])\n",
      "N = 70 \tWithout Noise = tensor(39.) \tWith Noise = tensor([-73.9398])\n",
      "N = 80 \tWithout Noise = tensor(41.) \tWith Noise = tensor([-11.3660])\n",
      "N = 90 \tWithout Noise = tensor(58.) \tWith Noise = tensor([100.2912])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this project we will:\n",
    "1) Create a query function that adds the right amount of noise to satisfy an epsilon constraint.\n",
    "2) Test it using a \"sum\" and \"mean\" query. Ensure that you use the correct sensitivity measures for both.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "def laplacian_noise(sensitivity, epsilon):\n",
    "    beta = sensitivity / epsilon\n",
    "    noise = torch.tensor(np.random.laplace(0, beta, 1)).float()\n",
    "    return noise\n",
    "\n",
    "# Sum query\n",
    "print(\"Sum query\")\n",
    "epsilon = 0.01             # Make cicles for varying amound of epsilon\n",
    "for i in range(1, 10):\n",
    "    db = create_db(10*i)\n",
    "    true_result = query_sum(db)\n",
    "    noise = laplacian_noise(1, epsilon)\n",
    "    private_result = true_result + noise\n",
    "    print(\"N =\", 10*i, \"\\tWithout Noise =\", true_result, \"\\tWith Noise =\", private_result)\n",
    "print(\"\")\n",
    "    \n",
    "# Mean query\n",
    "print(\"Mean query\")\n",
    "epsilon = 0.0001\n",
    "for i in range(1, 10):\n",
    "    db = create_db(10*i)\n",
    "    true_result = query_sum(db)\n",
    "    noise = laplacian_noise(1/(10*i), epsilon)\n",
    "    private_result = true_result + noise\n",
    "    print(\"N =\", 10*i, \"\\tWithout Noise =\", true_result, \"\\tWith Noise =\", private_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6: Differential Privacy for Deep Learning\n",
    "\n",
    "So in the last lessons you may have been wondering - what does all of this have to do with Deep Learning? Well, these same techniques we were just studying form the core primitives for how Differential Privacy provides guarantees in the context of Deep Learning. \n",
    "\n",
    "Previously, we defined perfect privacy as \"a query to a database returns the same value even if we remove any person from the database\", and used this intuition in the description of epsilon/delta. In the context of deep learning we have a similar standard.\n",
    "\n",
    "Training a model on a dataset should return the same model even if we remove any person from the dataset.\n",
    "\n",
    "Thus, we've replaced \"querying a database\" with \"training a model on a dataset\". In essence, the training process is a kind of query. However, one should note that this adds two points of complexity which database queries did not have:\n",
    "\n",
    "    1. do we always know where \"people\" are referenced in the dataset?\n",
    "    2. neural models rarely never train to the same output model, even on identical data\n",
    "\n",
    "The answer to (1) is to treat each training example as a single, separate person. Strictly speaking, this is often overly zealous as some training examples have no relevance to people and others may have multiple/partial (consider an image with multiple people contained within it). Thus, localizing exactly where \"people\" are referenced, and thus how much your model would change if people were removed, is challenging.\n",
    "\n",
    "The answer to (2) is also an open problem - but several interesitng proposals have been made. We're going to focus on one of the most popular proposals, PATE.\n",
    "\n",
    "## An Example Scenario: A Health Neural Network\n",
    "\n",
    "First we're going to consider a scenario - you work for a hospital and you have a large collection of images about your patients. However, you don't know what's in them. You would like to use these images to develop a neural network which can automatically classify them, however since your images aren't labeled, they aren't sufficient to train a classifier. \n",
    "\n",
    "However, being a cunning strategist, you realize that you can reach out to 10 partner hospitals which DO have annotated data. It is your hope to train your new classifier on their datasets so that you can automatically label your own. While these hospitals are interested in helping, they have privacy concerns regarding information about their patients. Thus, you will use the following technique to train a classifier which protects the privacy of patients in the other hospitals.\n",
    "\n",
    "- 1) You'll ask each of the 10 hospitals to train a model on their own datasets (All of which have the same kinds of labels)\n",
    "- 2) You'll then use each of the 10 partner models to predict on your local dataset, generating 10 labels for each of your datapoints\n",
    "- 3) Then, for each local data point (now with 10 labels), you will perform a DP query to generate the final true label. This query is a \"max\" function, where \"max\" is the most frequent label across the 10 labels. We will need to add laplacian noise to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 4) Finally, we will retrain a new model on our local dataset which now has labels. This will be our final \"DP\" model.\n",
    "\n",
    "So, let's walk through these steps. I will assume you're already familiar with how to train/predict a deep neural network, so we'll skip steps 1 and 2 and work with example data. We'll focus instead on step 3, namely how to perform the DP query for each example using toy data.\n",
    "\n",
    "So, let's say we have 10,000 training examples, and we've got 10 labels for each example (from our 10 \"teacher models\" which were trained directly on private data). Each label is chosen from a set of 10 possible labels (categories) for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teachers = 10 # we're working with 10 partner hospitals\n",
    "num_examples = 10000 # the size of OUR dataset\n",
    "num_labels = 10 # number of lablels for our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int).transpose(1,0) # fake predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = list()\n",
    "for an_image in preds:\n",
    "\n",
    "    label_counts = np.bincount(an_image, minlength=num_labels)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    beta = 1 / epsilon\n",
    "\n",
    "    for i in range(len(label_counts)):\n",
    "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "\n",
    "    new_label = np.argmax(label_counts)\n",
    "    \n",
    "    new_labels.append(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project - PATE Analysis\n",
    "\n",
    "For the final project for this section, you're going to be given a dataset which you need to use to train a DP model using this PATE method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([9, 9, 3, 6, 9, 9, 9, 9, 8, 2])\n",
    "counts = np.bincount(labels, minlength=10)\n",
    "query_result = np.argmax(counts)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: May not have used enough values of l. Increase 'moments' variable and run again.\n"
     ]
    }
   ],
   "source": [
    "num_teachers, num_examples, num_labels = (100, 100, 10)\n",
    "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int) #fake preds\n",
    "indices = (np.random.rand(num_examples) * num_labels).astype(int) # true answers\n",
    "\n",
    "preds[:,0:10] *= 0\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5)\n",
    "\n",
    "assert data_dep_eps < data_ind_eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: May not have used enough values of l. Increase 'moments' variable and run again.\n",
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 1.52655213289881\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0:50] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 11.756462732485115\n",
      "Data Dependent Epsilon: 0.9029013677789843\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5, moments=20)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
